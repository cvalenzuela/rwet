{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/cristobalvalenzuela/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/cristobalvalenzuela/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import codecs \n",
    "import pronouncing\n",
    "import nltk\n",
    "import re\n",
    "import glob\n",
    "import json\n",
    "from random import choice\n",
    "from gutenberg.acquire import load_etext\n",
    "from gutenberg.cleanup import strip_headers\n",
    "\n",
    "# helper functions from nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "tokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'data/bash.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f21833c924d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# bash docs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mbash_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/bash.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# python docs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'data/bash.txt'"
     ]
    }
   ],
   "source": [
    "# Source files\n",
    "\n",
    "# bash docs\n",
    "bash_docs = open('../data/bash.txt').read()\n",
    "\n",
    "# python docs\n",
    "python_docs = sorted(glob.glob('../data/python-2.7.13-docs-text/tutorial/*.txt'))\n",
    "python_raw = u\"\"\n",
    "# read every file and append it to corpus raw\n",
    "for element in python_docs:\n",
    "    with codecs.open(element,\"r\",\"utf-8\") as element_file:\n",
    "        python_raw += element_file.read()\n",
    "\n",
    "# ulyses\n",
    "ulyses = strip_headers(load_etext(4300)).strip()\n",
    "\n",
    "# replacers (not used for now)\n",
    "occupations = open('data/occupations.json').read()\n",
    "occupations = json.loads(occupations)\n",
    "\n",
    "# js docs\n",
    "js_docs = open('data/jsdocs.txt').read().decode('ascii', errors=\"replace\")\n",
    "\n",
    "# Assembly docs\n",
    "assembly_docs = open('data/assembler-guide.txt').read().decode('ascii', errors=\"replace\")\n",
    "\n",
    "# the list of words to use\n",
    "all_words = python_raw.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# functions to create\n",
    "\n",
    "def create_line(position):\n",
    "    temp_index = position\n",
    "    sentence = ''\n",
    "    while len(sentence.encode('utf-8')) < 32:\n",
    "        word = all_words[temp_index]\n",
    "        sentence = sentence + \" \" +  word\n",
    "        temp_index -= 1\n",
    "        \n",
    "    sentence32 = []\n",
    "    for word in sentence.split():\n",
    "        clean_word = word.replace(\".\", \"\")\n",
    "        clean_word = clean_word.replace(\" \", \"\")\n",
    "        clean_word = clean_word.replace(\",\", \"\")\n",
    "        clean_word = clean_word.lower()\n",
    "        sentence32.append(clean_word)\n",
    "    \n",
    "    return \" \".join(reversed(sentence32))\n",
    "    #words = all_words[position-5 : position+1]\n",
    "    #line = []\n",
    "    #for word in words:\n",
    "        #clean_word = word.replace(\".\", \"\")\n",
    "        #line.append(clean_word)\n",
    "   # return \" \".join(line)\n",
    "\n",
    "def two_sentences():\n",
    "    x1 = ''\n",
    "    x2 = ''\n",
    "    sentences = []\n",
    "    while len(x2) == 0:\n",
    "        x1_word = choice(all_words)\n",
    "        try:\n",
    "            x2_word = choice(pronouncing.rhymes(x1_word))\n",
    "        except:\n",
    "            x2_word = ''\n",
    "        if len(x2_word) > 0:\n",
    "            x1_index = all_words.index(x1_word)\n",
    "            try:\n",
    "                x2_index = all_words.index(x2_word)\n",
    "            except:\n",
    "                x2_index = 0\n",
    "            if x2_index != 0:\n",
    "                x1 = create_line(x1_index) \n",
    "                x2 = create_line(x2_index)\n",
    "                sentences.append(x1)\n",
    "                sentences.append(x2)\n",
    "    return sentences      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'all_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-73d0bc5c7470>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# The corpus can only be programming documentation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtwo_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtwo_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtwo_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-4ad0316f9650>\u001b[0m in \u001b[0;36mtwo_sentences\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mx1_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mx2_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpronouncing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrhymes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'all_words' is not defined"
     ]
    }
   ],
   "source": [
    "# An original sonnet is 14 lines, each being 10 syllables long. \n",
    "# The structure is: ab ab, cdcd, efef, gg \n",
    "\n",
    "# A bit sonnet is 2^4 lines, each being an average of 2^5 syllables long. \n",
    "# The structure is: aa, bb cc, dede, ff, gg\n",
    "# The corpus can only be programming documentation\n",
    "\n",
    "a = two_sentences()\n",
    "b = two_sentences()\n",
    "c = two_sentences()\n",
    "d = two_sentences()\n",
    "e = two_sentences()\n",
    "f = two_sentences()\n",
    "g = two_sentences()\n",
    "\n",
    "print a[0]\n",
    "print a[1]\n",
    "\n",
    "print b[0]\n",
    "print b[1]\n",
    "\n",
    "print c[0]\n",
    "print c[1]\n",
    "\n",
    "print d[0]\n",
    "print e[1]\n",
    "print d[0]\n",
    "print e[1]\n",
    "\n",
    "print f[0]\n",
    "print f[1]\n",
    "\n",
    "print g[0]\n",
    "print g[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
